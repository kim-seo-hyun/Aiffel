{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c84f77ef",
   "metadata": {},
   "source": [
    "## EXPLORATION_04\n",
    "\n",
    "### 인공지능 작사가 만들기\n",
    "\n",
    "- keyword: NLP(Natural Language Processing)/ 순환신경망(RNN) \n",
    "\n",
    "#### lubric\n",
    "1. 가사 텍스트 생성 모델이 정상적으로 동작하는가? 텍스트 제너레이션 결과가 그럴듯한 문장으로 생성된다\n",
    "2. 데이터의 전처리와 데이터셋 구성 과정이 체계적으로 진행되었는가? 특수문자 제거, 토크나이저 생성, 패딩처리 등의 과정이 빠짐없이 진행되었다\n",
    "3. 텍스트 생성모델이 안정적으로 학습되었는가? 텍스트 생성모델의 validation loss가 2.2 이하로 낮다\n",
    "\n",
    "#### to-do list\n",
    "1. 데이터 준비>데이터 정제\n",
    "2. 평가 데이터셋 분리> 인공지능 만들기> loss 시각화\n",
    "3. 모델 평가\n",
    "\n",
    "\n",
    "#### trial and error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979835f7",
   "metadata": {},
   "source": [
    "### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d0f2e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re \n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5c1610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof', 'You saw her bathing on the roof', 'Her beauty and the moonlight overthrew her', 'She tied you', 'To a kitchen chair', 'She broke your throne, and she cut your hair', 'And from your lips she drew the Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah You say I took the name in vain', \"I don't even know the name\"]\n"
     ]
    }
   ],
   "source": [
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "856e5333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue   # 길이가 0인 문장은 건너뛰기\n",
    "    if sentence[-1] == \":\": continue  # 문장의 끝이 : 인 문장은 건너뛰기\n",
    "\n",
    "    if idx > 9: break   # 일단 문장 10개만 확인\n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3ff311",
   "metadata": {},
   "source": [
    "#### 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9ed218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "#입력된 문장을\n",
    "#1. 소문자로 바꾸고, 양쪽 공백을 삭제\n",
    "#2. 특수문자 양쪽에 공백을 넣고\n",
    "#3. 여러개의 공백은 하나의 공백으로 바꾸기\n",
    "#4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꾸기\n",
    "#5. 다시 양쪽 공백 삭제\n",
    "#6. 문장 시작에는 <start>, 끝에는 <end>를 추가\n",
    "#7. 노래 구조 삭제 ex. [Outro]\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    sentence = re.sub(\"\\[.*\\]*\", \" \", sentence) # 7\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f39a216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#정제된 문장을 모으기\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뛰기\n",
    "    if len(sentence) == 0: continue # 길이가 0이거나 \n",
    "    if len(sentence.split()) > 15 and len(sentence.split()) < 3 : continue # 토큰이 3보다 작거나 15개가 넘는 문장은 건너뛰기\n",
    "        \n",
    " \n",
    "    # 정제를 하고 담기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "# 정제된 결과 확인\n",
    "print(len(corpus))\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88986d1",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df7237de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 단어장의 크기는 12,000 이상 으로 설정\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus로 tokenizer 내부의 단어장을 완성\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게함\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 15)  \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705fb1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2643 ...    0    0    0]\n",
      " [   2   35    7 ...   43    3    0]\n",
      " ...\n",
      " [   5   22    9 ...   10 1014    3]\n",
      " [  37   15 9061 ...  878  644    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f0720747340>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(175986, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor, tokenizer = tokenize(corpus)\n",
    "\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9ad32b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# 단어사전 확인\n",
    "\n",
    "len(tokenizer.index_word)\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break #단어장의 10번째 단어까지 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa721e6",
   "metadata": {},
   "source": [
    "### 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43c6d50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   64   57    9  970 6048    3    0    0    0]\n",
      "[  50    5   91  297   64   57    9  970 6048    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b99854bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (140788, 14)\n",
      "Target Train: (140788, 14)\n",
      "Source Val: (35198, 14)\n",
      "Target Val: (35198, 14)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state = 2)\n",
    "enc_val_train, enc_val_val, dec_val_train, dec_val_val = train_test_split(enc_train, dec_train, test_size = 0.125 , random_state = 2) \n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)\n",
    "print(\"Source Val:\", enc_val.shape)\n",
    "print(\"Target Val:\", dec_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2487d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(enc_val_train) #텐서의 1차원, 전체 문장의 개수\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 12000개 + 0:<pad>를 포함\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만들기\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfbf1f7",
   "metadata": {},
   "source": [
    "### 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b5d374",
   "metadata": {},
   "source": [
    "모델의 Embedding Size와 Hidden Size를 조절하며 10 Epoch 안에 val_loss 값을 2.2 수준으로 줄일 수 있는 모델을 설계하세요! (Loss는 아래 제시된 Loss 함수를 그대로 사용!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9918fc9",
   "metadata": {},
   "source": [
    "#### 모델의 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2fc1d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8877de",
   "metadata": {},
   "source": [
    "#### [Model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a5e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256 #단어 하나의 특징 수\n",
    "hidden_size = 2048 #퍼셉트론의 개수\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d51eb940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[ 1.62190452e-04, -2.20601542e-05,  4.90402635e-05, ...,\n",
       "         -1.82441028e-04, -2.68238800e-04,  3.12287535e-04],\n",
       "        [-3.48532558e-05, -2.89317017e-04,  7.54615685e-05, ...,\n",
       "         -4.18709969e-04, -5.91588963e-04,  4.60059557e-04],\n",
       "        [-3.15328616e-05, -2.75376806e-04,  1.44471283e-04, ...,\n",
       "         -2.53642036e-04, -7.68889731e-04,  5.78273030e-04],\n",
       "        ...,\n",
       "        [ 2.96007143e-04, -1.75639179e-05,  1.02908125e-04, ...,\n",
       "          3.12125077e-04, -1.14619143e-05,  5.82600420e-04],\n",
       "        [ 5.64836781e-04, -2.16517874e-04, -2.95500882e-04, ...,\n",
       "          1.92838546e-04, -1.25597198e-05,  7.79286551e-04],\n",
       "        [ 7.88917649e-04, -4.76359040e-04, -6.57383760e-04, ...,\n",
       "          1.26471918e-04, -1.15979601e-05,  9.70039051e-04]],\n",
       "\n",
       "       [[ 1.62190452e-04, -2.20601542e-05,  4.90402635e-05, ...,\n",
       "         -1.82441028e-04, -2.68238800e-04,  3.12287535e-04],\n",
       "        [ 1.77651644e-04, -4.82845826e-05, -4.48682113e-05, ...,\n",
       "         -4.54723719e-04, -3.18958657e-04,  6.14610675e-04],\n",
       "        [ 3.36971862e-04, -9.24882843e-05, -1.75245383e-04, ...,\n",
       "         -5.18999645e-04, -1.81804775e-04,  7.65804958e-04],\n",
       "        ...,\n",
       "        [ 2.32474194e-04, -9.49823516e-06,  8.75966507e-05, ...,\n",
       "          9.26316425e-04,  1.16455508e-03,  4.01861151e-04],\n",
       "        [ 3.84231185e-04, -1.34128801e-04, -3.14504985e-04, ...,\n",
       "          8.75656784e-04,  9.17701051e-04,  7.20285694e-04],\n",
       "        [ 5.71983575e-04, -2.64282833e-04, -7.26057042e-04, ...,\n",
       "          7.73082138e-04,  6.74654089e-04,  1.06175325e-03]],\n",
       "\n",
       "       [[ 1.62190452e-04, -2.20601542e-05,  4.90402635e-05, ...,\n",
       "         -1.82441028e-04, -2.68238800e-04,  3.12287535e-04],\n",
       "        [ 5.64444672e-05, -2.54326114e-05,  5.18302732e-05, ...,\n",
       "         -9.76412193e-05, -4.14860260e-04,  4.93404630e-04],\n",
       "        [-1.92599546e-04, -2.77454208e-04, -1.22470563e-04, ...,\n",
       "         -2.45670759e-04, -4.40067874e-04,  8.30704696e-04],\n",
       "        ...,\n",
       "        [-6.58257224e-04, -1.22670352e-03, -1.54042651e-03, ...,\n",
       "          3.02703033e-04, -1.30660759e-04,  1.50269642e-03],\n",
       "        [-3.65085580e-04, -1.44203799e-03, -1.66790350e-03, ...,\n",
       "          3.82876256e-04, -1.23789665e-04,  1.64852920e-03],\n",
       "        [-1.14823590e-04, -1.68905372e-03, -1.76918774e-03, ...,\n",
       "          4.87069919e-04, -1.23644888e-04,  1.74146867e-03]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 1.62190452e-04, -2.20601542e-05,  4.90402635e-05, ...,\n",
       "         -1.82441028e-04, -2.68238800e-04,  3.12287535e-04],\n",
       "        [ 2.59273278e-04, -3.63388273e-04,  1.37552648e-04, ...,\n",
       "          8.89977746e-05, -3.94354429e-04,  3.62080580e-04],\n",
       "        [ 5.56954532e-04, -5.37291344e-04,  1.77754409e-04, ...,\n",
       "          6.15415920e-04, -4.17856936e-04,  3.71809700e-04],\n",
       "        ...,\n",
       "        [ 1.10802369e-03, -1.85564184e-03, -8.70783930e-04, ...,\n",
       "          3.85466556e-04, -9.75585426e-04,  2.44082278e-03],\n",
       "        [ 1.22987852e-03, -2.02726573e-03, -1.06453127e-03, ...,\n",
       "          4.28735715e-04, -9.41631384e-04,  2.45277071e-03],\n",
       "        [ 1.30359596e-03, -2.21332442e-03, -1.23631768e-03, ...,\n",
       "          4.98177367e-04, -8.91291536e-04,  2.40773871e-03]],\n",
       "\n",
       "       [[ 1.62190452e-04, -2.20601542e-05,  4.90402635e-05, ...,\n",
       "         -1.82441028e-04, -2.68238800e-04,  3.12287535e-04],\n",
       "        [ 2.57624284e-04,  1.52884561e-04,  2.74467719e-04, ...,\n",
       "         -4.75175388e-04, -6.12476491e-04,  5.21089067e-04],\n",
       "        [ 2.24783536e-04,  1.91796076e-04,  5.12870552e-04, ...,\n",
       "         -3.97601631e-04, -6.33343821e-04,  5.45992982e-04],\n",
       "        ...,\n",
       "        [ 3.56915087e-04, -1.00626296e-03, -1.36305706e-03, ...,\n",
       "          9.92766581e-05,  2.95180565e-04,  1.61382521e-03],\n",
       "        [ 5.31040307e-04, -1.16242014e-03, -1.55078736e-03, ...,\n",
       "          1.24259881e-04,  2.01022907e-04,  1.81146013e-03],\n",
       "        [ 6.57959143e-04, -1.35594944e-03, -1.70255767e-03, ...,\n",
       "          1.96171823e-04,  1.28741187e-04,  1.95585098e-03]],\n",
       "\n",
       "       [[ 1.62190452e-04, -2.20601542e-05,  4.90402635e-05, ...,\n",
       "         -1.82441028e-04, -2.68238800e-04,  3.12287535e-04],\n",
       "        [ 3.71557078e-04,  6.14643477e-06, -2.51652800e-05, ...,\n",
       "         -2.67644180e-04, -6.34161232e-04,  6.04654429e-04],\n",
       "        [ 2.09134261e-04, -2.66667106e-04,  7.71217092e-05, ...,\n",
       "         -2.40027090e-04, -7.97666784e-04,  4.40661563e-04],\n",
       "        ...,\n",
       "        [ 6.08684088e-04, -1.29188353e-03, -3.39843915e-04, ...,\n",
       "          1.10033655e-03, -1.21012308e-04,  1.48382131e-03],\n",
       "        [ 8.38296604e-04, -1.40175561e-03, -5.92541997e-04, ...,\n",
       "          1.00070122e-03, -1.17235286e-04,  1.73074205e-03],\n",
       "        [ 1.00034429e-03, -1.55157316e-03, -8.23910639e-04, ...,\n",
       "          9.39805002e-04, -1.15069539e-04,  1.90186827e-03]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋에서 데이터 한 배치만 불러오는 방법\n",
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "# 한 배치만 불러온 데이터를 모델에 넣기\n",
    "model(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9376dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  18882560  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  33562624  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  24590049  \n",
      "=================================================================\n",
      "Total params: 80,107,489\n",
      "Trainable params: 80,107,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a6a6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "275/275 [==============================] - 258s 930ms/step - loss: 2.8778 - val_loss: 2.9051\n",
      "Epoch 2/10\n",
      "275/275 [==============================] - 261s 950ms/step - loss: 2.6902 - val_loss: 2.8049\n",
      "Epoch 3/10\n",
      "275/275 [==============================] - 262s 954ms/step - loss: 2.5114 - val_loss: 2.7169\n",
      "Epoch 4/10\n",
      "275/275 [==============================] - 262s 955ms/step - loss: 2.1613 - val_loss: 2.5841\n",
      "Epoch 6/10\n",
      "275/275 [==============================] - 263s 957ms/step - loss: 1.9977 - val_loss: 2.5340\n",
      "Epoch 7/10\n",
      "275/275 [==============================] - 267s 973ms/step - loss: 1.8419 - val_loss: 2.4975\n",
      "Epoch 8/10\n",
      "275/275 [==============================] - 268s 975ms/step - loss: 1.6954 - val_loss: 2.4699\n",
      "Epoch 9/10\n",
      "275/275 [==============================] - 264s 961ms/step - loss: 1.5577 - val_loss: 2.4452\n",
      "Epoch 10/10\n",
      "275/275 [==============================] - 262s 955ms/step - loss: 1.4318 - val_loss: 2.4310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f06a4798580>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, validation_data=(enc_val, dec_val),epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "26dcb762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        #1. 입력받은 문장의 텐서를 입력\n",
    "        predict = model(test_tensor) \n",
    "        #2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냅니다\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        #3. 2에서 예측된 word index를 문장 뒤에 붙입니다\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        #4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15bca8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> i love you , i love you <end> \n",
      "<start> you re the only one who ever drove me crazy <end> \n",
      "<start> a letter full of coke rental car from <unk> <end> \n",
      "<start> you make me wanna get all their rules <end> \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, tokenizer, init_sentence=\"<start> i love\"))\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> you re\"))\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> a letter\"))\n",
    "print(generate_text(model, tokenizer, init_sentence=\"<start> you make\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f51196",
   "metadata": {},
   "source": [
    "#### trial and error\n",
    "\n",
    " NLP라는 분야도 처음 접해보고 RNN이란 개념도 처음 공부해봤다. 이번 프로젝트는 사실 시작하기도 전부터 조금 잘해보고 싶었던 분야다. '인공지능 작사가 만들기' 자체가 흥미로웠고 그래서 더 욕심이 나 마지막까지 이러고 있다... \n",
    "하지만 생각보다 아직 내 실력이 부족해서 흥미로운 결과를 얻진 못했다. 따로 한글 가사 데이터셋을 구성해서 모델링을 해봤는데 생각보다 결과가 좋지 않았고 하이퍼 파라미터를 조정하는 부분도 미숙했기 때문이다. \n",
    "그래도 이제 루브릭 평가 지표를 2개를 만족시키는 학습은 3-4시간만 투자하면 얻어낼 수 있다는 사실에 약 한 달이 채 되지 않는 시간동안 많이 경험을 해본 것 같다. \n",
    "- 이번 프로젝트에서 토큰화를 처음 진행해 보았는데 다음에 기회가 된다면 데이터 전처리, 하이퍼 파라미터 조정에 대해 더 깊은 공부를 해보고 싶다.\n",
    "- num_words, batch size, embedding size, hidden size 등 수치 변화를 통해 validation loss를 맞추려고 여러차례 반복해봤는데 시간이 너무 오래걸려 결국 마지막 날 마지막 시간까지 조정하고 기다렸다. 아래에 삽입한 이미지는 근접했던 loss 값.\n",
    "- 마지막에 Total params: 257,794,017까지 나온건 잊지 못할 듯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
